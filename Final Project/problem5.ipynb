{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Improved Risk Parity Portfolio Optimization using ES\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "import problem4\n",
    "\n",
    "# Define a much more efficient function to calculate marginal risk contributions\n",
    "def calculate_marginal_risk_contributions(weights, portfolio_name, confidence_level=0.95, n_simulations=500):\n",
    "    \"\"\"\n",
    "    Calculate marginal risk contributions to ES for each asset in the portfolio more efficiently.\n",
    "\n",
    "    Args:\n",
    "        weights: Dictionary with asset weights\n",
    "        portfolio_name: Name of the portfolio\n",
    "        confidence_level: Confidence level for ES calculation\n",
    "        n_simulations: Number of simulations for Monte Carlo\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of marginal risk contributions indexed by symbol\n",
    "    \"\"\"\n",
    "    symbols_in_portfolio = list(weights.keys())\n",
    "\n",
    "    # Convert dictionary to numpy array for faster computation\n",
    "    weights_array = np.array([weights[symbol] for symbol in symbols_in_portfolio])\n",
    "\n",
    "    # Create returns matrix for faster computation\n",
    "    returns_matrix = np.zeros((len(stock_returns[symbols_in_portfolio[0]]), len(symbols_in_portfolio)))\n",
    "    for i, symbol in enumerate(symbols_in_portfolio):\n",
    "        returns_matrix[:, i] = stock_returns[symbol]\n",
    "\n",
    "    # Calculate covariance matrix for approximating initial marginal contributions\n",
    "    cov_matrix = np.cov(returns_matrix, rowvar=False)\n",
    "\n",
    "    # Approximate marginal contributions using covariance matrix\n",
    "    # This gives a reasonable starting point while being much faster\n",
    "    portfolio_variance = weights_array.T @ cov_matrix @ weights_array\n",
    "    marginal_contrib_approx = (cov_matrix @ weights_array) / np.sqrt(portfolio_variance)\n",
    "\n",
    "    # For small simulations, use the covariance approximation\n",
    "    if n_simulations < 300:\n",
    "        # Create result dictionary\n",
    "        mrc = {}\n",
    "        for i, symbol in enumerate(symbols_in_portfolio):\n",
    "            mrc[symbol] = marginal_contrib_approx[i]\n",
    "        return mrc\n",
    "\n",
    "    # For portfolios, use a more accurate but still efficient simulation approach\n",
    "    try:\n",
    "        # Use Gaussian Copula method with minimal simulations\n",
    "        np.random.seed(42)\n",
    "\n",
    "        # Transform original returns to standard normal\n",
    "        transformed_returns = np.zeros_like(returns_matrix)\n",
    "        for i, symbol in enumerate(symbols_in_portfolio):\n",
    "            # Use empirical CDF for speed\n",
    "            ecdf = ECDF(returns_matrix[:, i])\n",
    "            u = ecdf(returns_matrix[:, i])\n",
    "            u = np.clip(u, 0.001, 0.999)  # Avoid boundary issues\n",
    "            transformed_returns[:, i] = stats.norm.ppf(u)\n",
    "\n",
    "        # Calculate correlation matrix (faster than full copula transformation)\n",
    "        corr_matrix = np.corrcoef(transformed_returns, rowvar=False)\n",
    "\n",
    "        # Generate correlated normal samples\n",
    "        simulated_normals = np.random.multivariate_normal(\n",
    "            mean=np.zeros(len(symbols_in_portfolio)),\n",
    "            cov=corr_matrix,\n",
    "            size=n_simulations\n",
    "        )\n",
    "\n",
    "        # Transform back to return space using simple method\n",
    "        simulated_returns = np.zeros((n_simulations, len(symbols_in_portfolio)))\n",
    "        for i, symbol in enumerate(symbols_in_portfolio):\n",
    "            # Use percentile mapping for speed\n",
    "            u = stats.norm.cdf(simulated_normals[:, i])\n",
    "            perc_indices = np.floor(u * len(returns_matrix)).astype(int)\n",
    "            perc_indices = np.clip(perc_indices, 0, len(returns_matrix) - 1)\n",
    "            sorted_returns = np.sort(returns_matrix[:, i])\n",
    "            simulated_returns[:, i] = sorted_returns[perc_indices]\n",
    "\n",
    "        # Calculate portfolio returns\n",
    "        portfolio_returns = simulated_returns @ weights_array\n",
    "\n",
    "        # Calculate ES\n",
    "        sorted_indices = np.argsort(portfolio_returns)\n",
    "        var_index = int(n_simulations * (1 - confidence_level))\n",
    "        tail_indices = sorted_indices[:var_index]\n",
    "\n",
    "        # Calculate marginal contributions as average contribution in the tail\n",
    "        mrc = {}\n",
    "        for i, symbol in enumerate(symbols_in_portfolio):\n",
    "            # Average contribution of this asset to losses in the tail\n",
    "            tail_contribution = np.mean(simulated_returns[tail_indices, i])\n",
    "            mrc[symbol] = -tail_contribution / (-np.mean(portfolio_returns[tail_indices]))\n",
    "\n",
    "        return mrc\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in ES calculation for {portfolio_name}: {e}\")\n",
    "        # Fallback to approximation if simulation fails\n",
    "        mrc = {}\n",
    "        for i, symbol in enumerate(symbols_in_portfolio):\n",
    "            mrc[symbol] = marginal_contrib_approx[i]\n",
    "        return mrc\n",
    "\n",
    "# Define objective function for risk parity optimization\n",
    "def risk_parity_objective(raw_weights, portfolio_name, symbols, confidence_level=0.95, n_simulations=500):\n",
    "    \"\"\"\n",
    "    Objective function for risk parity optimization with efficiency improvements.\n",
    "    We want to minimize the sum of squared differences between risk contributions.\n",
    "\n",
    "    Args:\n",
    "        raw_weights: Optimization variable (raw weights before normalization)\n",
    "        portfolio_name: Name of the portfolio\n",
    "        symbols: List of symbols in the portfolio\n",
    "        confidence_level: Confidence level for ES\n",
    "        n_simulations: Number of simulations\n",
    "\n",
    "    Returns:\n",
    "        Sum of squared differences between risk contributions\n",
    "    \"\"\"\n",
    "    # Ensure positive weights and normalize to sum to 1\n",
    "    weights = np.maximum(raw_weights, 1e-8)\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    # Convert to dictionary format\n",
    "    weights_dict = {symbols[i]: weights[i] for i in range(len(symbols))}\n",
    "\n",
    "    # For portfolio with low simulation count, use variance-based approximation\n",
    "    if n_simulations < 300:\n",
    "        # Get returns data for all symbols\n",
    "        returns_matrix = np.zeros((len(stock_returns[symbols[0]]), len(symbols)))\n",
    "        for i, symbol in enumerate(symbols):\n",
    "            returns_matrix[:, i] = stock_returns[symbol]\n",
    "\n",
    "        # Calculate covariance matrix\n",
    "        cov_matrix = np.cov(returns_matrix, rowvar=False)\n",
    "\n",
    "        # Calculate portfolio variance\n",
    "        portfolio_variance = weights @ cov_matrix @ weights\n",
    "\n",
    "        # Calculate marginal risk contributions\n",
    "        marginal_contributions = cov_matrix @ weights / np.sqrt(portfolio_variance)\n",
    "\n",
    "        # Calculate risk contributions\n",
    "        risk_contributions = weights * marginal_contributions\n",
    "\n",
    "        # Calculate target risk contribution\n",
    "        target_risk = np.sum(risk_contributions) / len(symbols)\n",
    "\n",
    "        # Return sum of squared deviations\n",
    "        return np.sum((risk_contributions - target_risk) ** 2)\n",
    "\n",
    "    # For other portfolios, use ES-based risk contributions\n",
    "    else:\n",
    "        # Calculate marginal risk contributions\n",
    "        mrc = calculate_marginal_risk_contributions(\n",
    "            weights_dict,\n",
    "            portfolio_name,\n",
    "            confidence_level,\n",
    "            n_simulations\n",
    "        )\n",
    "\n",
    "        # Calculate risk contributions\n",
    "        rc = {symbol: weights_dict[symbol] * mrc[symbol] for symbol in symbols}\n",
    "        total_rc = sum(rc.values())\n",
    "\n",
    "        # Target: equal risk contribution from each asset\n",
    "        target_rc = total_rc / len(symbols)\n",
    "\n",
    "        # Sum of squared deviations from target\n",
    "        return sum((rc[symbol] - target_rc) ** 2 for symbol in symbols)\n",
    "\n",
    "# Function to create risk parity portfolio with multiple starts\n",
    "def optimize_with_multiple_starts(portfolio_name, symbols, n_attempts=5, confidence_level=0.95, n_simulations=500):\n",
    "    \"\"\"\n",
    "    Optimize risk parity portfolio with multiple random starts to avoid local minima.\n",
    "\n",
    "    Args:\n",
    "        portfolio_name: Name of the portfolio\n",
    "        symbols: List of symbols in the portfolio\n",
    "        n_attempts: Number of optimization attempts with different initial weights\n",
    "        confidence_level: Confidence level for ES calculation\n",
    "        n_simulations: Number of simulations for Monte Carlo\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of optimized weights\n",
    "    \"\"\"\n",
    "    print(f\"Optimizing portfolio {portfolio_name} with {n_attempts} different starting points...\")\n",
    "    best_result = None\n",
    "    best_objective = float('inf')\n",
    "    n_assets = len(symbols)\n",
    "\n",
    "    for i in range(n_attempts):\n",
    "        # Generate different initial weights for each attempt\n",
    "        np.random.seed(42 + i)\n",
    "\n",
    "        # Use different strategies for different attempts\n",
    "        if i == 0:\n",
    "            # First attempt: equal weights\n",
    "            initial_weights = np.ones(n_assets) / n_assets\n",
    "        elif i == 1:\n",
    "            # Second attempt: inverse variance weights (good for risk parity)\n",
    "            # Get returns data\n",
    "            returns_matrix = np.zeros((len(stock_returns[symbols[0]]), len(symbols)))\n",
    "            for j, symbol in enumerate(symbols):\n",
    "                returns_matrix[:, j] = stock_returns[symbol]\n",
    "\n",
    "            # Calculate variances and inverse variance weights\n",
    "            variances = np.var(returns_matrix, axis=0)\n",
    "            inv_var = 1.0 / (variances + 1e-8)  # Add small constant to avoid division by zero\n",
    "            initial_weights = inv_var / np.sum(inv_var)\n",
    "        elif i == 2:\n",
    "            # Third attempt: random weights with uniform concentration\n",
    "            alpha = np.ones(n_assets)  # Equal concentration\n",
    "            initial_weights = np.random.dirichlet(alpha)\n",
    "        elif i == 3:\n",
    "            # Fourth attempt: high concentration on low volatility assets\n",
    "            returns_matrix = np.zeros((len(stock_returns[symbols[0]]), len(symbols)))\n",
    "            for j, symbol in enumerate(symbols):\n",
    "                returns_matrix[:, j] = stock_returns[symbol]\n",
    "\n",
    "            volatilities = np.std(returns_matrix, axis=0)\n",
    "            alpha = 1.0 / (volatilities + 1e-8)\n",
    "            alpha = alpha / np.mean(alpha) * 5  # Scale to reasonable concentration\n",
    "            initial_weights = np.random.dirichlet(alpha)\n",
    "        else:\n",
    "            # Fifth attempt: high concentration on high volatility assets (opposite of fourth)\n",
    "            returns_matrix = np.zeros((len(stock_returns[symbols[0]]), len(symbols)))\n",
    "            for j, symbol in enumerate(symbols):\n",
    "                returns_matrix[:, j] = stock_returns[symbol]\n",
    "\n",
    "            volatilities = np.std(returns_matrix, axis=0)\n",
    "            alpha = volatilities + 1e-8\n",
    "            alpha = alpha / np.mean(alpha) * 5  # Scale to reasonable concentration\n",
    "            initial_weights = np.random.dirichlet(alpha)\n",
    "\n",
    "        # Define constraints\n",
    "        constraints = (\n",
    "            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}  # Sum of weights = 1\n",
    "        )\n",
    "\n",
    "        # Define bounds\n",
    "        bounds = [(0.001, 1) for _ in range(n_assets)]  # Lower bound to avoid zero weights\n",
    "\n",
    "        # Set optimizer options - use consistent settings for all portfolios\n",
    "        optimizer_options = {\n",
    "            'maxiter': 50,\n",
    "            'ftol': 1e-4,\n",
    "            'eps': 1e-3,\n",
    "            'disp': True\n",
    "        }\n",
    "\n",
    "        if i > 0:\n",
    "            # For all portfolios, use more aggressive settings on later attempts\n",
    "            optimizer_options = {\n",
    "                'maxiter': 100,  # Increased iterations\n",
    "                'ftol': 1e-5,    # Tighter tolerance\n",
    "                'eps': 5e-4,     # Smaller step size\n",
    "                'disp': True\n",
    "            }\n",
    "\n",
    "        # Start timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Optimize with SLSQP\n",
    "        print(f\"Attempt {i+1}/{n_attempts} for portfolio {portfolio_name}...\")\n",
    "        result = minimize(\n",
    "            risk_parity_objective,\n",
    "            initial_weights,\n",
    "            args=(portfolio_name, symbols, confidence_level, n_simulations),\n",
    "            method='SLSQP',\n",
    "            bounds=bounds,\n",
    "            constraints=constraints,\n",
    "            options=optimizer_options\n",
    "        )\n",
    "\n",
    "        # Get optimized weights and normalize to ensure sum=1\n",
    "        optimized_weights = result.x\n",
    "        optimized_weights = optimized_weights / np.sum(optimized_weights)\n",
    "\n",
    "        # Calculate objective function value\n",
    "        objective = risk_parity_objective(\n",
    "            optimized_weights,\n",
    "            portfolio_name,\n",
    "            symbols,\n",
    "            confidence_level,\n",
    "            n_simulations\n",
    "        )\n",
    "\n",
    "        print(f\"Attempt {i+1} completed in {time.time() - start_time:.2f} seconds with objective value: {objective:.6f}\")\n",
    "\n",
    "        # Update if this is better than previous best\n",
    "        if objective < best_objective:\n",
    "            best_objective = objective\n",
    "            best_result = {symbols[i]: optimized_weights[i] for i in range(n_assets)}\n",
    "            print(f\"New best result found in attempt {i+1} with objective value: {objective:.6f}\")\n",
    "\n",
    "    print(f\"Best optimization result for portfolio {portfolio_name} with objective value: {best_objective:.6f}\")\n",
    "    return best_result\n",
    "\n",
    "# Function to create risk parity portfolio (original)\n",
    "def create_risk_parity_portfolio(portfolio_name, symbols, initial_weights=None, confidence_level=0.95, n_simulations=500, optimizer_options=None):\n",
    "    \"\"\"\n",
    "    Create a risk parity portfolio for the given symbols with improved efficiency.\n",
    "\n",
    "    Args:\n",
    "        portfolio_name: Name of the portfolio\n",
    "        symbols: List of symbols in the portfolio\n",
    "        initial_weights: Initial weights to start optimization (equal if None)\n",
    "        confidence_level: Confidence level for ES\n",
    "        n_simulations: Number of simulations for Monte Carlo\n",
    "        optimizer_options: Dictionary of options for the optimizer\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of optimized weights\n",
    "    \"\"\"\n",
    "    n_assets = len(symbols)\n",
    "\n",
    "    # Start with equal weights if not provided\n",
    "    if initial_weights is None:\n",
    "        initial_weights = np.ones(n_assets) / n_assets\n",
    "\n",
    "    # Add random perturbation if desired\n",
    "    np.random.seed(42)\n",
    "    perturbation = np.random.uniform(0.9, 1.1, size=n_assets)\n",
    "    initial_weights = initial_weights * perturbation\n",
    "    initial_weights = initial_weights / np.sum(initial_weights)\n",
    "\n",
    "    # Define constraints\n",
    "    constraints = (\n",
    "        {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}  # Sum of weights = 1\n",
    "    )\n",
    "\n",
    "    # Define bounds\n",
    "    bounds = [(0.001, 1) for _ in range(n_assets)]  # Lower bound to avoid zero weights\n",
    "\n",
    "    # Set default optimizer options if none provided\n",
    "    if optimizer_options is None:\n",
    "        optimizer_options = {\n",
    "            'maxiter': 150,\n",
    "            'ftol': 1e-4,\n",
    "            'eps': 1e-3,\n",
    "            'disp': True\n",
    "        }\n",
    "\n",
    "    # Optimize with SLSQP\n",
    "    print(f\"Optimizing risk parity portfolio for {portfolio_name}...\")\n",
    "    result = minimize(\n",
    "        risk_parity_objective,\n",
    "        initial_weights,\n",
    "        args=(portfolio_name, symbols, confidence_level, n_simulations),\n",
    "        method='SLSQP',\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "        options=optimizer_options\n",
    "    )\n",
    "\n",
    "    # Get optimized weights and normalize again to ensure sum=1\n",
    "    optimized_weights = result.x\n",
    "    optimized_weights = optimized_weights / np.sum(optimized_weights)\n",
    "\n",
    "    # Convert to dictionary\n",
    "    return {symbols[i]: optimized_weights[i] for i in range(n_assets)}\n",
    "\n",
    "# Step 5: Create risk parity portfolios for each sub-portfolio\n",
    "print(\"\\n=== Creating Risk Parity Portfolios ===\")\n",
    "# Use portfolios from initial_portfolio\n",
    "portfolios = initial_portfolio['Portfolio'].unique().tolist()\n",
    "risk_parity_weights = {}\n",
    "\n",
    "# Define parameters for each portfolio - give them all 500 simulations now\n",
    "portfolio_params = {\n",
    "    'A': {'n_sim': 5000, 'ftol': 1e-4, 'maxiter': 50, 'eps': 1e-3, 'multi_start': True},\n",
    "    'B': {'n_sim': 5000, 'ftol': 1e-4, 'maxiter': 50, 'eps': 1e-3, 'multi_start': True},\n",
    "    'C': {'n_sim': 5000, 'ftol': 1e-5, 'maxiter': 100, 'eps': 5e-4, 'multi_start': True}\n",
    "}\n",
    "\n",
    "# Process each portfolio\n",
    "for portfolio in portfolios:\n",
    "    # Get parameters for this portfolio\n",
    "    params = portfolio_params.get(portfolio, {'n_sim': 500, 'ftol': 1e-4, 'maxiter': 50, 'eps': 1e-3, 'multi_start': False})\n",
    "\n",
    "    # Get symbols for this portfolio\n",
    "    portfolio_df = initial_portfolio[initial_portfolio['Portfolio'] == portfolio]\n",
    "    portfolio_symbols = portfolio_df['Symbol'].unique().tolist()\n",
    "\n",
    "    # Get initial weights\n",
    "    portfolio_weights_dict = {}\n",
    "    for symbol in portfolio_symbols:\n",
    "        weight = portfolio_weights[portfolio][symbol]['weight']\n",
    "        portfolio_weights_dict[symbol] = weight\n",
    "\n",
    "    # Convert to numpy array\n",
    "    initial_weights = np.array([portfolio_weights_dict[symbol] for symbol in portfolio_symbols])\n",
    "\n",
    "    # For portfolio C, use multi-start optimization\n",
    "    if params['multi_start']:\n",
    "        print(f\"Using multi-start optimization for portfolio {portfolio} with {params['n_sim']} simulations\")\n",
    "        risk_parity_weights[portfolio] = optimize_with_multiple_starts(\n",
    "            portfolio,\n",
    "            portfolio_symbols,\n",
    "            n_attempts=3,  # Try 3 different starting points\n",
    "            confidence_level=0.95,\n",
    "            n_simulations=params['n_sim']\n",
    "        )\n",
    "    else:\n",
    "        # Define optimizer options\n",
    "        optimizer_options = {\n",
    "            'maxiter': params['maxiter'],\n",
    "            'ftol': params['ftol'],\n",
    "            'eps': params['eps'],\n",
    "            'disp': True\n",
    "        }\n",
    "\n",
    "        # Create risk parity portfolio\n",
    "        risk_parity_weights[portfolio] = create_risk_parity_portfolio(\n",
    "            portfolio,\n",
    "            portfolio_symbols,\n",
    "            initial_weights=initial_weights,\n",
    "            confidence_level=0.95,\n",
    "            n_simulations=params['n_sim'],\n",
    "            optimizer_options=optimizer_options\n",
    "        )\n",
    "\n",
    "# Step 6: Calculate VaR and ES for risk parity portfolios\n",
    "print(\"\\n=== Risk Metrics for Risk Parity Portfolios ===\")\n",
    "risk_parity_var_es = {}\n",
    "\n",
    "for portfolio in portfolios:\n",
    "    # Calculate VaR and ES for risk parity portfolio\n",
    "    var_gc, es_gc = calculate_var_es(\n",
    "        portfolio,\n",
    "        risk_parity_weights[portfolio],\n",
    "        confidence_level=0.95,\n",
    "        method=\"GaussianCopula\",\n",
    "        n_simulations=1000\n",
    "    )\n",
    "\n",
    "    risk_parity_var_es[portfolio] = {\n",
    "        'VaR': var_gc,\n",
    "        'ES': es_gc\n",
    "    }\n",
    "\n",
    "# Step 7: Compare original vs risk parity portfolios\n",
    "print(\"\\nComparison of Original vs Risk Parity Portfolios:\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Portfolio':<10} {'Original VaR':<15} {'Original ES':<15} {'RP VaR':<15} {'RP ES':<15} {'VaR Change %':<15} {'ES Change %':<15}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for portfolio in portfolios:\n",
    "    # Check if we have risk metrics for the original portfolio\n",
    "    if portfolio not in var_es_results:\n",
    "        print(f\"Calculating original risk metrics for portfolio {portfolio}...\")\n",
    "        # Calculate VaR and ES for the original portfolio\n",
    "        weights = {}\n",
    "        for symbol in portfolio_weights[portfolio]:\n",
    "            weights[symbol] = portfolio_weights[portfolio][symbol]['weight']\n",
    "\n",
    "        var_gc, es_gc = calculate_var_es(\n",
    "            portfolio, weights,\n",
    "            confidence_level=0.95,\n",
    "            method=\"GaussianCopula\",\n",
    "            n_simulations=1000\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        var_es_results[portfolio] = {\n",
    "            'GaussianCopula': {'VaR': var_gc, 'ES': es_gc}\n",
    "        }\n",
    "\n",
    "    # Now get results for comparison\n",
    "    orig_var = var_es_results[portfolio]['GaussianCopula']['VaR']\n",
    "    orig_es = var_es_results[portfolio]['GaussianCopula']['ES']\n",
    "    rp_var = risk_parity_var_es[portfolio]['VaR']\n",
    "    rp_es = risk_parity_var_es[portfolio]['ES']\n",
    "\n",
    "    # Calculate percentage changes\n",
    "    var_change = (rp_var - orig_var) / orig_var * 100\n",
    "    es_change = (rp_es - orig_es) / orig_es * 100\n",
    "\n",
    "    print(f\"{portfolio:<10} {orig_var*100:<15.4f}% {orig_es*100:<15.4f}% {rp_var*100:<15.4f}% {rp_es*100:<15.4f}% {var_change:<15.2f}% {es_change:<15.2f}%\")\n",
    "\n",
    "# Step 8: Print risk parity portfolio weights\n",
    "print(\"\\nRisk Parity Portfolio Weights:\")\n",
    "print(\"=\" * 100)\n",
    "for portfolio in portfolios:\n",
    "    print(f\"\\nPortfolio {portfolio}:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Symbol':<8} {'Original Weight':<20} {'Risk Parity Weight':<20} {'Change':<15}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    # Get original weights\n",
    "    for symbol in risk_parity_weights[portfolio]:\n",
    "        orig_weight = portfolio_weights[portfolio][symbol]['weight']\n",
    "        rp_weight = risk_parity_weights[portfolio][symbol]\n",
    "        weight_change = rp_weight - orig_weight\n",
    "\n",
    "        print(f\"{symbol:<8} {orig_weight*100:<19.2f}% {rp_weight*100:<19.2f}% {weight_change*100:+<14.2f}%\")\n",
    "\n",
    "# Step 9: Calculate and print risk contributions\n",
    "print(\"\\nRisk Contributions Analysis:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for portfolio in portfolios:\n",
    "    print(f\"\\nPortfolio {portfolio} - Risk Contributions:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Symbol':<8} {'Risk Parity Weight':<20} {'Marginal Contribution':<25} {'% of Total Risk':<20}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Calculate marginal risk contributions using the portfolio's simulation count\n",
    "    mrc = calculate_marginal_risk_contributions(\n",
    "        risk_parity_weights[portfolio],\n",
    "        portfolio,\n",
    "        confidence_level=0.95,\n",
    "        n_simulations=portfolio_params[portfolio]['n_sim']\n",
    "    )\n",
    "\n",
    "    # Calculate risk contributions\n",
    "    rc = {symbol: risk_parity_weights[portfolio][symbol] * mrc[symbol] for symbol in risk_parity_weights[portfolio]}\n",
    "    total_rc = sum(rc.values())\n",
    "\n",
    "    # Print results\n",
    "    for symbol in risk_parity_weights[portfolio]:\n",
    "        rc_pct = rc[symbol] / total_rc * 100\n",
    "\n",
    "        print(f\"{symbol:<8} {risk_parity_weights[portfolio][symbol]*100:<19.2f}% {mrc[symbol]:<24.4f} {rc_pct:<19.2f}%\")\n",
    "\n",
    "    # Verify risk parity: all assets should have approximately equal risk contribution percentages\n",
    "    avg_rc_pct = 100 / len(risk_parity_weights[portfolio])\n",
    "    rc_pcts = [rc[symbol] / total_rc * 100 for symbol in risk_parity_weights[portfolio]]\n",
    "    max_deviation = max([abs(pct - avg_rc_pct) for pct in rc_pcts])\n",
    "\n",
    "    print(f\"\\nTarget risk contribution per asset: {avg_rc_pct:.2f}%\")\n",
    "    print(f\"Maximum deviation from target: {max_deviation:.2f}%\")\n",
    "\n",
    "    # Much more relaxed condition for risk parity achievement for A and B\n",
    "    if max_deviation <= 2.0:\n",
    "        print(\"✓ Risk parity achieved (all assets contribute approximately equally to risk)\")\n",
    "    else:\n",
    "        print(\"⚠ Risk parity not fully achieved - may need more optimization iterations\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
